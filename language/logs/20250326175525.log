2025-03-26 17:55:25,069 - __main__ - INFO - Logging to logs/language_model_20250326.log
2025-03-26 17:55:25,069 - __main__ - INFO - Starting main execution
2025-03-26 17:55:25,069 - __main__ - INFO - Downloading dataset from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
2025-03-26 17:55:25,413 - __main__ - INFO - Dataset downloaded and saved
2025-03-26 17:55:25,413 - __main__ - INFO - Loaded text data with length: 1115394
2025-03-26 17:55:25,413 - __main__ - INFO - Dataset split - Train: 892315, Val: 111539, Test: 111540
2025-03-26 17:55:25,413 - __main__ - INFO - Initialized BPETokenizer
2025-03-26 17:55:25,413 - __main__ - INFO - Starting tokenizer training with desired vocab size: 1000
2025-03-26 17:55:25,422 - __main__ - INFO - Initial vocabulary size: 65, Number of merges required: 935
2025-03-26 17:55:25,592 - __main__ - INFO - Merge 0: Added token 'e ', Current vocab size: 66
2025-03-26 17:55:39,089 - __main__ - INFO - Merge 100: Added token 'that ', Current vocab size: 166
2025-03-26 17:55:50,809 - __main__ - INFO - Merge 200: Added token 'O:
', Current vocab size: 266
2025-03-26 17:56:01,448 - __main__ - INFO - Merge 300: Added token 'ive ', Current vocab size: 366
2025-03-26 17:56:11,573 - __main__ - INFO - Merge 400: Added token '.

S', Current vocab size: 466
2025-03-26 17:56:21,294 - __main__ - INFO - Merge 500: Added token ' them', Current vocab size: 566
2025-03-26 17:56:30,763 - __main__ - INFO - Merge 600: Added token ' for', Current vocab size: 666
2025-03-26 17:56:40,029 - __main__ - INFO - Merge 700: Added token 'lord, ', Current vocab size: 766
2025-03-26 17:56:49,109 - __main__ - INFO - Merge 800: Added token 'QUEEN ELIZAB', Current vocab size: 866
2025-03-26 17:56:58,115 - __main__ - INFO - Merge 900: Added token 'been', Current vocab size: 966
2025-03-26 17:57:01,138 - __main__ - INFO - Tokenizer training completed. Final vocab size: 1000
2025-03-26 17:57:01,143 - __main__ - INFO - Configuration created with parameters: {'vocab_size': 500, 'hidden_size': 256, 'num_layers': 4, 'num_heads': 4, 'latent_dim': 64, 'block_size': 64, 'max_position_embeddings': 512, 'dropout': 0.1, 'ff_expansion_factor': 2, 'rms_norm_eps': 1e-06, 'learning_rate': 0.0005, 'weight_decay': 0.01, 'batch_size': 8, 'gradient_accumulation_steps': 4, 'num_epochs': 1, 'warmup_steps': 500, 'seq_length': 64, 'max_gen_length': 50, 'top_k': 50, 'z_loss_base': 1e-05, 'precision': 'fp32'}
2025-03-26 17:57:01,143 - __main__ - INFO - Trainer initialized with device: mps
2025-03-26 17:57:01,153 - __main__ - INFO - Using MPS (Apple Silicon GPU)
2025-03-26 17:57:01,154 - __main__ - INFO - Loading tokenized data from cache: cache/train_tokens.pt
2025-03-26 17:57:01,170 - __main__ - INFO - Loading tokenized data from cache: cache/val_tokens.pt
2025-03-26 17:57:01,172 - __main__ - INFO - Loading tokenized data from cache: cache/test_tokens.pt
2025-03-26 17:57:01,174 - __main__ - INFO - Tokenization complete - Train tokens: 336848, Val tokens: 44051, Test tokens: 45303
2025-03-26 17:57:01,174 - __main__ - INFO - Datasets created - Train samples: 336784, Val samples: 43987, Test samples: 45239
2025-03-26 17:57:01,174 - __main__ - INFO - Adjusted sequence length to 64 based on data size
2025-03-26 17:57:01,174 - __main__ - INFO - Dataloaders created - Train batches: 42098, Val batches: 5499, Test batches: 5655
2025-03-26 17:57:01,197 - __main__ - INFO - Model initialized
2025-03-26 17:57:01,215 - __main__ - INFO - Model moved to device: mps
2025-03-26 17:57:01,215 - __main__ - INFO - Skipping torch.compile as it's not supported on mps
2025-03-26 17:57:01,215 - __main__ - INFO - Using precision: fp32
2025-03-26 17:57:01,215 - __main__ - INFO - Model converted to fp32
2025-03-26 17:57:01,516 - __main__ - INFO - Starting training for 1 epochs with fp32 precision
2025-03-26 17:57:01,886 - __main__ - INFO - Epoch 1, Step 0: Loss = 1.7419, LR = 0.000001
2025-03-26 17:57:13,613 - __main__ - INFO - Epoch 1, Step 500: Loss = 1.3383, LR = 0.000500
2025-03-26 17:57:25,218 - __main__ - INFO - Epoch 1, Step 1000: Loss = 1.1504, LR = 0.000500
2025-03-26 17:57:36,972 - __main__ - INFO - Epoch 1, Step 1500: Loss = 1.0831, LR = 0.000499
2025-03-26 17:57:48,748 - __main__ - INFO - Epoch 1, Step 2000: Loss = 1.0425, LR = 0.000498
2025-03-26 17:58:00,523 - __main__ - INFO - Epoch 1, Step 2500: Loss = 1.0271, LR = 0.000497
2025-03-26 17:58:12,294 - __main__ - INFO - Epoch 1, Step 3000: Loss = 0.9565, LR = 0.000496
2025-03-26 17:58:24,112 - __main__ - INFO - Epoch 1, Step 3500: Loss = 0.9428, LR = 0.000494
2025-03-26 17:58:35,890 - __main__ - INFO - Epoch 1, Step 4000: Loss = 0.8914, LR = 0.000491
2025-03-26 17:58:47,689 - __main__ - INFO - Epoch 1, Step 4500: Loss = 0.8615, LR = 0.000489
2025-03-26 17:58:59,422 - __main__ - INFO - Epoch 1, Step 5000: Loss = 0.9122, LR = 0.000486
2025-03-26 17:59:11,331 - __main__ - INFO - Epoch 1, Step 5500: Loss = 0.8439, LR = 0.000482
2025-03-26 17:59:23,166 - __main__ - INFO - Epoch 1, Step 6000: Loss = 0.8450, LR = 0.000479
2025-03-26 17:59:35,046 - __main__ - INFO - Epoch 1, Step 6500: Loss = 0.8432, LR = 0.000475
2025-03-26 17:59:46,820 - __main__ - INFO - Epoch 1, Step 7000: Loss = 0.7953, LR = 0.000470
2025-03-26 17:59:58,707 - __main__ - INFO - Epoch 1, Step 7500: Loss = 0.8063, LR = 0.000466
2025-03-26 18:00:10,507 - __main__ - INFO - Epoch 1, Step 8000: Loss = 0.7637, LR = 0.000461
2025-03-26 18:00:22,367 - __main__ - INFO - Epoch 1, Step 8500: Loss = 0.7713, LR = 0.000456
2025-03-26 18:00:34,183 - __main__ - INFO - Epoch 1, Step 9000: Loss = 0.7637, LR = 0.000450
2025-03-26 18:00:46,081 - __main__ - INFO - Epoch 1, Step 9500: Loss = 0.7545, LR = 0.000444
2025-03-26 18:00:57,968 - __main__ - INFO - Epoch 1, Step 10000: Loss = 0.7422, LR = 0.000438
2025-03-26 18:01:09,881 - __main__ - INFO - Epoch 1, Step 10500: Loss = 0.7960, LR = 0.000432
2025-03-26 18:01:21,747 - __main__ - INFO - Epoch 1, Step 11000: Loss = 0.7267, LR = 0.000425
2025-03-26 18:01:33,650 - __main__ - INFO - Epoch 1, Step 11500: Loss = 0.7163, LR = 0.000419
2025-03-26 18:01:45,545 - __main__ - INFO - Epoch 1, Step 12000: Loss = 0.7267, LR = 0.000411
2025-03-26 18:01:57,387 - __main__ - INFO - Epoch 1, Step 12500: Loss = 0.6691, LR = 0.000404
2025-03-26 18:02:09,141 - __main__ - INFO - Epoch 1, Step 13000: Loss = 0.7108, LR = 0.000397
2025-03-26 18:02:20,946 - __main__ - INFO - Epoch 1, Step 13500: Loss = 0.6856, LR = 0.000389
2025-03-26 18:02:32,662 - __main__ - INFO - Epoch 1, Step 14000: Loss = 0.6683, LR = 0.000381
2025-03-26 18:02:44,471 - __main__ - INFO - Epoch 1, Step 14500: Loss = 0.6229, LR = 0.000373
2025-03-26 18:02:56,422 - __main__ - INFO - Epoch 1, Step 15000: Loss = 0.6861, LR = 0.000364
2025-03-26 18:03:08,377 - __main__ - INFO - Epoch 1, Step 15500: Loss = 0.5966, LR = 0.000356
2025-03-26 18:03:20,074 - __main__ - INFO - Epoch 1, Step 16000: Loss = 0.6224, LR = 0.000347
2025-03-26 18:03:31,816 - __main__ - INFO - Epoch 1, Step 16500: Loss = 0.6090, LR = 0.000339
2025-03-26 18:03:43,592 - __main__ - INFO - Epoch 1, Step 17000: Loss = 0.5761, LR = 0.000330
2025-03-26 18:03:55,399 - __main__ - INFO - Epoch 1, Step 17500: Loss = 0.6073, LR = 0.000321
2025-03-26 18:04:07,348 - __main__ - INFO - Epoch 1, Step 18000: Loss = 0.5872, LR = 0.000312
2025-03-26 18:04:19,228 - __main__ - INFO - Epoch 1, Step 18500: Loss = 0.6328, LR = 0.000302
2025-03-26 18:04:31,150 - __main__ - INFO - Epoch 1, Step 19000: Loss = 0.5447, LR = 0.000293
2025-03-26 18:04:43,060 - __main__ - INFO - Epoch 1, Step 19500: Loss = 0.5910, LR = 0.000284
2025-03-26 18:04:55,031 - __main__ - INFO - Epoch 1, Step 20000: Loss = 0.5741, LR = 0.000274
2025-03-26 18:05:06,936 - __main__ - INFO - Epoch 1, Step 20500: Loss = 0.5969, LR = 0.000265
2025-03-26 18:05:18,888 - __main__ - INFO - Epoch 1, Step 21000: Loss = 0.5694, LR = 0.000256
2025-03-26 18:05:30,749 - __main__ - INFO - Epoch 1, Step 21500: Loss = 0.6032, LR = 0.000246
2025-03-26 18:05:42,668 - __main__ - INFO - Epoch 1, Step 22000: Loss = 0.5749, LR = 0.000237
2025-03-26 18:05:54,518 - __main__ - INFO - Epoch 1, Step 22500: Loss = 0.5724, LR = 0.000227
2025-03-26 18:06:06,448 - __main__ - INFO - Epoch 1, Step 23000: Loss = 0.5750, LR = 0.000218
2025-03-26 18:06:18,330 - __main__ - INFO - Epoch 1, Step 23500: Loss = 0.5835, LR = 0.000209
2025-03-26 18:06:30,210 - __main__ - INFO - Epoch 1, Step 24000: Loss = 0.4973, LR = 0.000199
2025-03-26 18:06:42,127 - __main__ - INFO - Epoch 1, Step 24500: Loss = 0.5676, LR = 0.000190
2025-03-26 18:06:53,994 - __main__ - INFO - Epoch 1, Step 25000: Loss = 0.5445, LR = 0.000181
2025-03-26 18:07:05,934 - __main__ - INFO - Epoch 1, Step 25500: Loss = 0.4643, LR = 0.000172
2025-03-26 18:07:17,822 - __main__ - INFO - Epoch 1, Step 26000: Loss = 0.5348, LR = 0.000163
2025-03-26 18:07:29,752 - __main__ - INFO - Epoch 1, Step 26500: Loss = 0.4862, LR = 0.000154
2025-03-26 18:07:41,624 - __main__ - INFO - Epoch 1, Step 27000: Loss = 0.5320, LR = 0.000146
2025-03-26 18:07:53,542 - __main__ - INFO - Epoch 1, Step 27500: Loss = 0.5212, LR = 0.000137
2025-03-26 18:08:05,469 - __main__ - INFO - Epoch 1, Step 28000: Loss = 0.4926, LR = 0.000129
2025-03-26 18:08:17,385 - __main__ - INFO - Epoch 1, Step 28500: Loss = 0.5005, LR = 0.000121
2025-03-26 18:08:29,241 - __main__ - INFO - Epoch 1, Step 29000: Loss = 0.4948, LR = 0.000113
2025-03-26 18:08:41,116 - __main__ - INFO - Epoch 1, Step 29500: Loss = 0.4596, LR = 0.000105
2025-03-26 18:08:52,965 - __main__ - INFO - Epoch 1, Step 30000: Loss = 0.5420, LR = 0.000097
2025-03-26 18:09:04,868 - __main__ - INFO - Epoch 1, Step 30500: Loss = 0.4727, LR = 0.000090
2025-03-26 18:09:16,756 - __main__ - INFO - Epoch 1, Step 31000: Loss = 0.4590, LR = 0.000083
2025-03-26 18:09:28,595 - __main__ - INFO - Epoch 1, Step 31500: Loss = 0.4813, LR = 0.000076
2025-03-26 18:09:40,435 - __main__ - INFO - Epoch 1, Step 32000: Loss = 0.4686, LR = 0.000069
2025-03-26 18:09:52,342 - __main__ - INFO - Epoch 1, Step 32500: Loss = 0.4619, LR = 0.000063
2025-03-26 18:10:04,178 - __main__ - INFO - Epoch 1, Step 33000: Loss = 0.4648, LR = 0.000057
2025-03-26 18:10:16,090 - __main__ - INFO - Epoch 1, Step 33500: Loss = 0.4379, LR = 0.000051
2025-03-26 18:10:27,950 - __main__ - INFO - Epoch 1, Step 34000: Loss = 0.4251, LR = 0.000045
2025-03-26 18:10:39,866 - __main__ - INFO - Epoch 1, Step 34500: Loss = 0.5065, LR = 0.000040
2025-03-26 18:10:51,732 - __main__ - INFO - Epoch 1, Step 35000: Loss = 0.4710, LR = 0.000035
2025-03-26 18:11:03,667 - __main__ - INFO - Epoch 1, Step 35500: Loss = 0.4708, LR = 0.000030
2025-03-26 18:11:15,563 - __main__ - INFO - Epoch 1, Step 36000: Loss = 0.4804, LR = 0.000026
2025-03-26 18:11:27,440 - __main__ - INFO - Epoch 1, Step 36500: Loss = 0.4318, LR = 0.000022
2025-03-26 18:11:39,356 - __main__ - INFO - Epoch 1, Step 37000: Loss = 0.4247, LR = 0.000018
2025-03-26 18:11:51,314 - __main__ - INFO - Epoch 1, Step 37500: Loss = 0.4536, LR = 0.000015
2025-03-26 18:12:03,252 - __main__ - INFO - Epoch 1, Step 38000: Loss = 0.4630, LR = 0.000012
2025-03-26 18:12:15,188 - __main__ - INFO - Epoch 1, Step 38500: Loss = 0.4809, LR = 0.000009
2025-03-26 18:12:27,089 - __main__ - INFO - Epoch 1, Step 39000: Loss = 0.4602, LR = 0.000007
2025-03-26 18:12:39,085 - __main__ - INFO - Epoch 1, Step 39500: Loss = 0.4506, LR = 0.000005
2025-03-26 18:12:51,001 - __main__ - INFO - Epoch 1, Step 40000: Loss = 0.4356, LR = 0.000003
2025-03-26 18:13:02,914 - __main__ - INFO - Epoch 1, Step 40500: Loss = 0.4625, LR = 0.000002
2025-03-26 18:13:14,876 - __main__ - INFO - Epoch 1, Step 41000: Loss = 0.4628, LR = 0.000001
2025-03-26 18:13:26,796 - __main__ - INFO - Epoch 1, Step 41500: Loss = 0.4924, LR = 0.000000
2025-03-26 18:13:38,692 - __main__ - INFO - Epoch 1, Step 42000: Loss = 0.4895, LR = 0.000000
2025-03-26 18:13:41,006 - __main__ - INFO - Epoch 1 completed - Average training loss: 2.5506
2025-03-26 18:13:41,007 - __main__ - INFO - Starting evaluation on 5499 batches
2025-03-26 18:14:25,250 - __main__ - INFO - Evaluation completed - Average loss: 4.7393, Perplexity: 114.3504
2025-03-26 18:14:25,250 - __main__ - INFO - Epoch 1 - Validation loss: 4.7393, Perplexity: 114.3504
2025-03-26 18:14:25,250 - __main__ - INFO - Training completed
2025-03-26 18:14:25,251 - __main__ - INFO - Starting evaluation on 5655 batches
2025-03-26 18:15:10,468 - __main__ - INFO - Evaluation completed - Average loss: 5.3944, Perplexity: 220.1722
2025-03-26 18:15:10,468 - __main__ - INFO - Final test results - Loss: 5.3944, Perplexity: 220.1722
2025-03-26 18:15:10,499 - __main__ - INFO - Model saved to model.pth
2025-03-26 18:15:10,499 - __main__ - INFO - Generating text with prompt: 'To be or not to be'
2025-03-26 18:15:19,065 - __main__ - INFO - Generated text: To be or not to be;
Being novon, not unavoided women man
Anoise their about their own point, as if
Then we may choose him as he seek o'
2025-03-26 18:15:19,065 - __main__ - INFO - Execution completed
